import json
from app.config import NAVER_CLIENT_ID, NAVER_CLIENT_SECRET
import requests
from urllib.parse import urlparse
from bs4 import BeautifulSoup
import re
import time
from email.utils import parsedate_to_datetime


news_media_mapping = {
    "yonhapnews.co.kr": "ì—°í•©ë‰´ìŠ¤",
    "yonhapnewstv.co.kr": "ì—°í•©ë‰´ìŠ¤TV",
    "news1.kr": "ë‰´ìŠ¤1",
    "edu.donga.com": "ë™ì•„ì¼ë³´",
    "biz.heraldcorp.com": "í—¤ëŸ´ë“œê²½ì œ",
    "daily.hankooki.com": "í•œêµ­ì¼ë³´",
    "kmib.co.kr": "ê¸°ë…êµì¼ë³´",
    "kbs.co.kr": "KBS",
    "munhwa.com": "ë¬¸í™”ì¼ë³´",
    "sports.naver.com": "ë„¤ì´ë²„ ìŠ¤í¬ì¸ ",
    "newsis.com": "ë‰´ì‹œìŠ¤",
    "segye.com": "ì„¸ê³„ì¼ë³´",
    "hankooki.com": "í•œê²¨ë ˆ",
    "chosun.com": "ì¡°ì„ ì¼ë³´",
    "joongang.co.kr": "ì¤‘ì•™ì¼ë³´",
    "khan.co.kr": "ê²½í–¥ì‹ ë¬¸",
    "hani.co.kr": "í•œê²¨ë ˆ",
    "sports.chosun.com": "ìŠ¤í¬ì¸ ì¡°ì„ ",
    "sports.donga.com": "ìŠ¤í¬ì¸ ë™ì•„",
    "news.naver.com": "ë„¤ì´ë²„ ë‰´ìŠ¤",
    "n.news.naver.com": "ë„¤ì´ë²„ ë‰´ìŠ¤",
    "m.post.naver.com": "ë„¤ì´ë²„ í¬ìŠ¤íŠ¸",
    "seoul.co.kr": "ì„œìš¸ì‹ ë¬¸",
    "etnews.com": "ì „ìì‹ ë¬¸",
    "ytn.co.kr": "YTN",
    "tbs.seoul.kr": "TBS",
    "mbn.co.kr": "MBN ë‰´ìŠ¤",
    "news.kmib.co.kr": "êµ­ë¯¼ì¼ë³´",
    "newdaily.co.kr": "ë‰´ë°ì¼ë¦¬",
    "yna.co.kr": "ì—°í•©ë‰´ìŠ¤",
    "naeil.com": "ë‚´ì¼ì‹ ë¬¸",
    "kihoilbo.co.kr": "ê¸°í˜¸ì¼ë³´",
    "edaily.co.kr": "ì´ë°ì¼ë¦¬",
    "fnnews.com": "íŒŒì´ë„¨ì…œë‰´ìŠ¤",
    "hankyung.com": "í•œê²½",
    "incheonnews.com": "ì¸ì²œë‰´ìŠ¤",
    "bloter.net": "BROTER",
    "dt.co.kr": "ë””ì§€í„¸íƒ€ì„ìŠ¤",
    "sentv.co.kr": "ì„œìš¸ê²½ì œTV" ,
    "econovill.com": "ì´ì½”ë…¸ë¯¹ ë¦¬ë·°",
    "nytimes.com": "The New York Times",
    "digitaltoday.co.kr": "Digital Today",
    "koreajoongangdaily.joins.com": "Korea JoongAng Daily",
    "ddaily.co.kr": "ë””ì§€í„¸ ë°ì¼ë¦¬",
    "hankyung.com": "í•œêµ­ê²½ì œì‹ ë¬¸",
    "sedaily.com": "ì„œìš¸ê²½ì œ",
    "sportsseoul.com": "ìŠ¤í¬ì¸ ì„œìš¸",
    "joongboo.com": "ì¤‘ë¶€ì¼ë³´",
    "nocutnews.co.kr": "ë…¸ì»·ë‰´ìŠ¤",
    "imbc.com": "MBC ë‰´ìŠ¤",
    "news.mt.co.kr": "ë¨¸ë‹ˆíˆ¬ë°ì´",
}

# URLì—ì„œ ë„ë©”ì¸ ì¶”ì¶œ í•¨ìˆ˜
def extract_domain(url):
    parsed_url = urlparse(url)
    domain = parsed_url.hostname
    if domain.startswith("www."):
        domain = domain[4:]  # "www." ì œê±°
    return domain

# íƒœê·¸ ì œê±°
def clean_html_tags(text):
    return re.sub(r"<.*?>", "", text)

# ë‚ ì§œ í˜•ì‹ ì§€ì •
def format_date(pub_date_str):
    try:
        dt = parsedate_to_datetime(pub_date_str)
        return dt.strftime("%Y-%m-%d %H:%M")  # ì˜ˆ: 2025-05-06 10:59
    except Exception as e:
        print(f"ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {e}")
        return pub_date_str


def get_article_content(url: str) -> str:
    """
    ì£¼ì–´ì§„ URLì—ì„œ ê¸°ì‚¬ ë³¸ë¬¸ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜.

    :param url: ë‰´ìŠ¤ ê¸°ì‚¬ í˜ì´ì§€ì˜ URL
    :return: ê¸°ì‚¬ ë³¸ë¬¸ í…ìŠ¤íŠ¸ (ì‚¬ì§„ì€ ì œì™¸ë¨)
    """
    try:
        # ì›¹ í˜ì´ì§€ ìš”ì²­ ë³´ë‚´ê¸°
        response = requests.get(url)

        # ìš”ì²­ì´ ì„±ê³µì ì¼ ë•Œ
        if response.status_code == 200:
            # BeautifulSoupì„ ì‚¬ìš©í•´ HTML íŒŒì‹±
            soup = BeautifulSoup(response.text, 'html.parser')

            # ê¸°ì‚¬ ë³¸ë¬¸ì´ ë“¤ì–´ìˆëŠ” <article> íƒœê·¸ ì°¾ê¸°
            article = soup.find('article', {'id': 'dic_area', 'class': 'go_trans _article_content'})

            # ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ
            if article:
                content = article.get_text(strip=True)
                return content
            else:
                return "ê¸°ì‚¬ ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        else:
            return f"ì›¹ì‚¬ì´íŠ¸ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}"

    except Exception as e:
        return f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
    
    
# ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ APIë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘
def fetch_news(total, keywords):
    # ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸
    news_data = []
    start = 1
    
    # ìš”ì²­ URLì„ ìœ„í•œ ê¸°ë³¸ ì„¤ì •
    base_url = "https://openapi.naver.com/v1/search/news.json"

    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }

    total_news_count = total
    display_count = 100
    total_pages = total_news_count // display_count

    news_data = []

    for keyword in keywords:
        print(f"ğŸ” '{keyword}' í‚¤ì›Œë“œ ìˆ˜ì§‘ ì‹œì‘, í˜„ì¬ê¹Œì§€ ë¦¬ìŠ¤íŠ¸ ê¸¸ì´: {len(news_data)}")

        for start in range(1, total_pages + 1):
            params = {
                "query": keyword,
                "display": display_count,
                "start": (start - 1) * display_count + 1,
                "sort": "sim"
            }

            try:
                response = requests.get(base_url, headers=headers, params=params)
                data = response.json()

                for item in data["items"]:
                    if item["link"].startswith("https://n.news.naver.com/"):
                        news_item = {
                            "title": clean_html_tags(item["title"]),
                            "date": format_date(item["pubDate"]),
                            "link": item["link"],
                            "content": get_article_content(item["link"]),
                            "journal": news_media_mapping.get(extract_domain(item["originallink"]), "Unknown")
                        }
                        news_data.append(news_item)

                time.sleep(1)

            except Exception as e:
                print(f"âŒ '{keyword}' í‚¤ì›Œë“œì˜ {start}í˜ì´ì§€ì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}")
                break
    # "journal" ê°’ì´ "Unknown"ì´ ì•„ë‹Œ ë°ì´í„°ë§Œ ë‚¨ê¹€
    filtered_data = [item for item in news_data if item['journal'] != 'Unknown']

    # ì œëª© ê¸°ì¤€ ì¤‘ë³µ ì œê±°
    seen_titles = set()
    deduplicated_data = []
    for item in filtered_data:
        if item['title'] not in seen_titles:
            seen_titles.add(item['title'])
            deduplicated_data.append(item)

    # deduplicated_dataë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
    with open('data/news_data.json', 'w', encoding='utf-8') as f:
        json.dump(deduplicated_data, f, ensure_ascii=False, indent=4)

    print(f"âœ… ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ! ì´ ì €ì¥ëœ ê¸°ì‚¬ ìˆ˜: {len(deduplicated_data)}")
        
    return filtered_data 